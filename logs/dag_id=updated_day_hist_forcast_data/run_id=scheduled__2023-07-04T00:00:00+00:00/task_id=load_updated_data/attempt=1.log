[2023-07-05T08:51:52.398+0100] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: updated_day_hist_forcast_data.load_updated_data scheduled__2023-07-04T00:00:00+00:00 [queued]>
[2023-07-05T08:51:52.406+0100] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: updated_day_hist_forcast_data.load_updated_data scheduled__2023-07-04T00:00:00+00:00 [queued]>
[2023-07-05T08:51:52.406+0100] {taskinstance.py:1308} INFO - Starting attempt 1 of 3
[2023-07-05T08:51:52.416+0100] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): load_updated_data> on 2023-07-04 00:00:00+00:00
[2023-07-05T08:51:52.421+0100] {standard_task_runner.py:57} INFO - Started process 10850 to run task
[2023-07-05T08:51:52.425+0100] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'updated_day_hist_forcast_data', 'load_updated_data', 'scheduled__2023-07-04T00:00:00+00:00', '--job-id', '148', '--raw', '--subdir', '/home/haboubi/Desktop/api/API_weather/dags/update_day_hist.py', '--cfg-path', '/tmp/tmpcv2ybe9s']
[2023-07-05T08:51:52.426+0100] {standard_task_runner.py:85} INFO - Job 148: Subtask load_updated_data
[2023-07-05T08:51:52.489+0100] {task_command.py:410} INFO - Running <TaskInstance: updated_day_hist_forcast_data.load_updated_data scheduled__2023-07-04T00:00:00+00:00 [running]> on host haboubi-VivoBook-15-ASUS-Laptop-X540UB
[2023-07-05T08:51:52.586+0100] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haboubi' AIRFLOW_CTX_DAG_ID='updated_day_hist_forcast_data' AIRFLOW_CTX_TASK_ID='load_updated_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-04T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-04T00:00:00+00:00'
[2023-07-05T08:51:52.593+0100] {logging_mixin.py:149} INFO - debug prb: Paris
[2023-07-05T08:51:52.638+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.639+0100] {logging_mixin.py:149} INFO - debug prb: London
[2023-07-05T08:51:52.660+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.661+0100] {logging_mixin.py:149} INFO - debug prb: Barcelona
[2023-07-05T08:51:52.679+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.679+0100] {logging_mixin.py:149} INFO - debug prb: Rome
[2023-07-05T08:51:52.697+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.697+0100] {logging_mixin.py:149} INFO - debug prb: New York
[2023-07-05T08:51:52.715+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.716+0100] {logging_mixin.py:149} INFO - debug prb: Tokyo
[2023-07-05T08:51:52.734+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.735+0100] {logging_mixin.py:149} INFO - debug prb: Istanbul
[2023-07-05T08:51:52.752+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.752+0100] {logging_mixin.py:149} INFO - debug prb: Dubai
[2023-07-05T08:51:52.770+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.771+0100] {logging_mixin.py:149} INFO - debug prb: Sydney
[2023-07-05T08:51:52.788+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.789+0100] {logging_mixin.py:149} INFO - debug prb: Bangkok
[2023-07-05T08:51:52.807+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.808+0100] {logging_mixin.py:149} INFO - debug prb: Amsterdam
[2023-07-05T08:51:52.825+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.825+0100] {logging_mixin.py:149} INFO - debug prb: Rio De Janeiro
[2023-07-05T08:51:52.843+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.844+0100] {logging_mixin.py:149} INFO - debug prb: Prague
[2023-07-05T08:51:52.866+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.867+0100] {logging_mixin.py:149} INFO - debug prb: Cape Town
[2023-07-05T08:51:52.890+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.891+0100] {logging_mixin.py:149} INFO - debug prb: Buenos Aires
[2023-07-05T08:51:52.912+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.912+0100] {logging_mixin.py:149} INFO - debug prb: Berlin
[2023-07-05T08:51:52.930+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.931+0100] {logging_mixin.py:149} INFO - debug prb: Singapore
[2023-07-05T08:51:52.949+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.950+0100] {logging_mixin.py:149} INFO - debug prb: Florence
[2023-07-05T08:51:52.978+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:52.978+0100] {logging_mixin.py:149} INFO - debug prb: San Francisco
[2023-07-05T08:51:53.001+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:53.002+0100] {logging_mixin.py:149} INFO - debug prb: Marrakech
[2023-07-05T08:51:53.018+0100] {logging_mixin.py:149} INFO - last_time_updated: 2023-07-04
[2023-07-05T08:51:53.025+0100] {python.py:183} INFO - Done. Returned value was: 0
[2023-07-05T08:51:53.057+0100] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=updated_day_hist_forcast_data, task_id=load_updated_data, execution_date=20230704T000000, start_date=20230705T075152, end_date=20230705T075153
[2023-07-05T08:51:53.078+0100] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-05T08:51:53.105+0100] {taskinstance.py:2653} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-07-05T09:01:49.085+0100] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: updated_day_hist_forcast_data.load_updated_data scheduled__2023-07-04T00:00:00+00:00 [queued]>
[2023-07-05T09:01:49.095+0100] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: updated_day_hist_forcast_data.load_updated_data scheduled__2023-07-04T00:00:00+00:00 [queued]>
[2023-07-05T09:01:49.095+0100] {taskinstance.py:1308} INFO - Starting attempt 1 of 3
[2023-07-05T09:01:49.105+0100] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): load_updated_data> on 2023-07-04 00:00:00+00:00
[2023-07-05T09:01:49.110+0100] {standard_task_runner.py:57} INFO - Started process 14840 to run task
[2023-07-05T09:01:49.113+0100] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'updated_day_hist_forcast_data', 'load_updated_data', 'scheduled__2023-07-04T00:00:00+00:00', '--job-id', '167', '--raw', '--subdir', '/home/haboubi/Desktop/api/API_weather/dags/update_day_hist.py', '--cfg-path', '/tmp/tmpr87he64t']
[2023-07-05T09:01:49.115+0100] {standard_task_runner.py:85} INFO - Job 167: Subtask load_updated_data
[2023-07-05T09:01:49.179+0100] {task_command.py:410} INFO - Running <TaskInstance: updated_day_hist_forcast_data.load_updated_data scheduled__2023-07-04T00:00:00+00:00 [running]> on host haboubi-VivoBook-15-ASUS-Laptop-X540UB
[2023-07-05T09:01:49.291+0100] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haboubi' AIRFLOW_CTX_DAG_ID='updated_day_hist_forcast_data' AIRFLOW_CTX_TASK_ID='load_updated_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-04T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-04T00:00:00+00:00'
[2023-07-05T09:01:49.297+0100] {logging_mixin.py:149} INFO - debug prb: Paris
[2023-07-05T09:01:49.317+0100] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/haboubi/Desktop/api/API_weather/api_env/lib/python3.10/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/haboubi/Desktop/api/API_weather/api_env/lib/python3.10/site-packages/airflow/operators/python.py", line 198, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/haboubi/Desktop/api/API_weather/dags/update_day_hist.py", line 59, in load_updated_data_db
    loc=update_location_entite(loc)
  File "/home/haboubi/Desktop/api/API_weather/dags/update_day_hist.py", line 38, in update_location_entite
    hist_days_csv=pd.read_csv(directory_hist,sep='\t')
  File "/home/haboubi/Desktop/api/API_weather/api_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/home/haboubi/Desktop/api/API_weather/api_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/home/haboubi/Desktop/api/API_weather/api_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/home/haboubi/Desktop/api/API_weather/api_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/home/haboubi/Desktop/api/API_weather/api_env/lib/python3.10/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'city_hist_updated/Paris.csv'
[2023-07-05T09:01:49.333+0100] {taskinstance.py:1345} INFO - Marking task as UP_FOR_RETRY. dag_id=updated_day_hist_forcast_data, task_id=load_updated_data, execution_date=20230704T000000, start_date=20230705T080149, end_date=20230705T080149
[2023-07-05T09:01:49.343+0100] {standard_task_runner.py:104} ERROR - Failed to execute job 167 for task load_updated_data ([Errno 2] No such file or directory: 'city_hist_updated/Paris.csv'; 14840)
[2023-07-05T09:01:49.366+0100] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-05T09:01:49.395+0100] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
